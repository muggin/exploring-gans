{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "\n",
    "from utils import *\n",
    "from skimage import transform \n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from keras import initializers\n",
    "from keras.layers import Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Nadam\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data constants\n",
    "max_size = (32, 32, 1)\n",
    "data_files = {'cifar': '../data/cifar.npy',\n",
    "              'mnist': '../data/mnist.npy'}\n",
    "\n",
    "# load and transform data\n",
    "data = np.load(data_files['mnist'])\n",
    "data = data.astype('float32')\n",
    "\n",
    "if data.shape[1:3] > max_size:\n",
    "    data = np.array([transform.resize(image, max_size, preserve_range=True, order=0)] \n",
    "                    for image in data)\n",
    "\n",
    "# shuffle data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# get data sizes\n",
    "data_count = data.shape[0]\n",
    "data_size = data.shape[1:4] if data.shape[3] > 1 else data.shape[1:3]\n",
    "data_dim = reduce(operator.mul, data.shape[1:])\n",
    "\n",
    "# reshape data if necessary\n",
    "data = data.reshape(data_count, *data_size)\n",
    "\n",
    "print 'Loaded data {}'.format(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "data_reshaped = data.reshape(data_count, -1)\n",
    "data_mean = np.mean(data_reshaped, axis=1, keepdims=True, dtype=np.float64)\n",
    "data_z_mean = (data_reshaped - data_mean)\n",
    "data_std = np.std(data_z_mean, axis=1, keepdims=True, dtype=np.float64) + 1e-16\n",
    "data_normed = data_z_mean / data_std\n",
    "data_proc = data_normed.reshape(data_count, *data_size)\n",
    "\n",
    "print 'Before preproc', data.mean(), data.std()\n",
    "print 'After preproc', data_proc.mean(), data_proc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# latent space generators\n",
    "def get_uniform_space(high, low, space_size):\n",
    "    return lambda batch_size: np.random.uniform(low, high, (batch_size, space_size)).astype('float32')\n",
    "\n",
    "def get_gaussian_space(mean, var, space_size):\n",
    "    return lambda batch_size: np.random.normal(mean, var, (batch_size, space_size)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mlp_model(data_dim, latent_dim, sgd_lr=0.001, sgd_mom=0.1, gen_activation='tanh', relu_alpha=0.01, do_p=0.05):\n",
    "    # setup optimizer\n",
    "    opt = SGD(lr=sgd_lr, momentum=sgd_mom)\n",
    "\n",
    "    leaky_alpha = relu_alpha \n",
    "    dropout_p = do_p \n",
    "    # setup generator network\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(256, input_dim=latent_dim))\n",
    "    generator.add(LeakyReLU(alpha=leaky_alpha))\n",
    "    generator.add(Dense(512))\n",
    "    generator.add(LeakyReLU(alpha=leaky_alpha))\n",
    "    generator.add(Dense(1024))\n",
    "    generator.add(LeakyReLU(alpha=leaky_alpha))\n",
    "    generator.add(Dense(data_dim, activation=gen_activation))\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "    # setup discriminator network\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(1024, input_dim=data_dim))\n",
    "    discriminator.add(LeakyReLU(alpha=leaky_alpha))\n",
    "    discriminator.add(Dropout(dropout_p))\n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(alpha=leaky_alpha))\n",
    "    discriminator.add(Dropout(dropout_p))\n",
    "    discriminator.add(Dense(256))\n",
    "    discriminator.add(LeakyReLU(alpha=leaky_alpha))\n",
    "    discriminator.add(Dropout(dropout_p))\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "    # setup combined network\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(latent_dim,))\n",
    "    gan_output = discriminator(generator(gan_input))\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return (generator, discriminator, gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup experiment\n",
    "max_epochs = 25\n",
    "batch_size = 64\n",
    "latent_dim = 100\n",
    "z_space = get_gaussian_space(0, 1, latent_dim)\n",
    "generator, discriminator, gan = get_mlp_model(data_dim, latent_dim)\n",
    "\n",
    "# prepare data\n",
    "dis_labels = np.random.uniform(0.85, 1.1, 2*batch_size) #np.ones(2*batch_size) * 0.9 \n",
    "dis_labels[:batch_size] = 0 \n",
    "gan_labels = np.ones(batch_size)# * 0.9\n",
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "using_batch_norm = False\n",
    "\n",
    "# pre-train discriminator \n",
    "# for batch_ix in tqdm(xrange(0, data_count-batch_size, batch_size)):\n",
    "#     z_samples = z_space(batch_size).astype('float32')\n",
    "#     discriminator.trainable = False\n",
    "#     g_loss = gan.train_on_batch(z_samples, gan_labels)\n",
    "#     discriminator.trainable = True\n",
    "#     x_samples = data_proc[batch_ix: batch_ix+batch_size].reshape(batch_size, -1)\n",
    "#     d_loss = discriminator.train_on_batch(x_samples, dis_labels[batch_size:])\n",
    "\n",
    "for epoch_ix in xrange(max_epochs):\n",
    "    np.random.shuffle(data_proc)\n",
    "    \n",
    "    g_losses_epoch = []\n",
    "    d_losses_epoch = []\n",
    "    for batch_ix in tqdm(xrange(0, data_count-batch_size, batch_size)):\n",
    "        if using_batch_norm:\n",
    "            # train discriminator - true\n",
    "            discriminator.trainable = True\n",
    "            x_samples = data_proc[batch_ix: batch_ix+batch_size].reshape(batch_size, -1)\n",
    "            d_loss_t = discriminator.train_on_batch(\n",
    "                x_samples, dis_labels[batch_size:])\n",
    "\n",
    "            # train discriminator - fake\n",
    "            z_samples = z_space(batch_size).astype('float32')\n",
    "            d_loss_f = discriminator.train_on_batch(\n",
    "                np.vstack(generator.predict(z_samples)), dis_labels[:batch_size])\n",
    "            d_loss = (d_loss_t + d_loss_f) / 2\n",
    "        else:\n",
    "            # train discriminator\n",
    "            discriminator.trainable = True\n",
    "            z_samples = z_space(batch_size).astype('float32')\n",
    "            x_samples = data_proc[batch_ix: batch_ix+batch_size].reshape(batch_size, -1)\n",
    "            d_loss = discriminator.train_on_batch(\n",
    "                np.vstack([generator.predict(z_samples), x_samples]), dis_labels)\n",
    "        \n",
    "        # train generator\n",
    "        z_samples = z_space(batch_size).astype('float32')\n",
    "        discriminator.trainable = False\n",
    "        g_loss = gan.train_on_batch(z_samples, gan_labels)\n",
    "        \n",
    "        g_losses_epoch.append(g_loss)\n",
    "        d_losses_epoch.append(d_loss)\n",
    "    \n",
    "    # compute average losses in prev epoch and store them in the history\n",
    "    g_loss_avg = reduce(lambda x, y: x + y, g_losses_epoch) / len(g_loss_epoch)\n",
    "    d_loss_avg = reduce(lambda x, y: x + y, d_losses_epoch) / len(d_loss_epoch)\n",
    "    g_losses.append(g_loss_avg)\n",
    "    d_losses.append(d_loss_avg)\n",
    "    \n",
    "    print 'epoch: {} -- loss G: {} - D: {}'.format(epoch_ix, \n",
    "                                                   g_loss_avg, \n",
    "                                                   d_loss_avg)\n",
    "    z_samples = z_space(batch_size).astype('float32')\n",
    "    fakes = generator.predict(z_samples[:8,:])\n",
    "    plot_images(fakes, data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_performance(('g_loss', g_losses), ('d_loss', d_losses))\n",
    "\n",
    "z_samples = z_space(batch_size).astype('float32')\n",
    "fakes = generator.predict(z_samples[:8])\n",
    "plot_images(fakes, data_size);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
