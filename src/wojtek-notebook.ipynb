{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "\n",
    "from skimage import transform \n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# share GPU\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.49\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images, image_dims, max_col=4, file_name='', to_file=False):\n",
    "    rows = (len(images)-1)// max_col + 1\n",
    "    for ix, image in enumerate(images):\n",
    "        plt.subplot(rows, max_col, ix+1)\n",
    "        plt.imshow(image.reshape(image_dims), cmap='gray_r', interpolation=None)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_performance(*metrics):\n",
    "    \"\"\"\n",
    "    function plots various performance metrics over time.\n",
    "    provide multiple tuples in the format (metric-name, metric).\n",
    "    different metrics (tuples) should be provided as consecutive arguments\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    for label, metric in metrics:\n",
    "        epochs = range(len(metric))\n",
    "        plt.plot(metric, label=label)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data constants\n",
    "max_size = (32, 32, 1)\n",
    "data_files = {'cifar': '../data/cifar.npy',\n",
    "              'mnist': '../data/mnist.npy',\n",
    "              'lfw': '../data/lfw.npy'}\n",
    "\n",
    "# load and transform data\n",
    "data = np.load(data_files['mnist'])\n",
    "data = data.astype('float32')\n",
    "\n",
    "if data.shape[1:3] > max_size:\n",
    "    data = np.array([transform.resize(image, max_size, preserve_range=True, order=0)] \n",
    "                    for image in data)\n",
    "\n",
    "# shuffle data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# \n",
    "data_count = data.shape[0]\n",
    "data_size = data.shape[1:4] if data.shape[3] > 1 else data.shape[1:3]\n",
    "data_dim = reduce(operator.mul, data.shape[1:])\n",
    "\n",
    "print 'Loaded data {}'.format(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# latent space generators\n",
    "def get_uniform_space(high, low, space_size):\n",
    "    return lambda batch_size: np.random.uniform(low, high, (batch_size, space_size)).astype('float32')\n",
    "\n",
    "def get_gaussian_space(mean, var, space_size):\n",
    "    return lambda batch_size: np.random.normal(mean, var, (batch_size, space_size)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mlp_model(input_dim):\n",
    "    # setup optimizer\n",
    "    d_opt = SGD(lr=0.01, momentum=0.1)\n",
    "    g_opt = Adam(lr=0.01)\n",
    "\n",
    "    leaky_alpha = 0.001\n",
    "    # setup generator network\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(2048*2, input_dim=input_dim))\n",
    "    generator.add(LeakyReLU(alpha=leaky_alpha))\n",
    "    generator.add(Dense(1024*2))\n",
    "    generator.add(LeakyReLU(alpha=leaky_alpha))\n",
    "    generator.add(Dense(data_dim, activation='linear'))\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=g_opt)\n",
    "\n",
    "    # setup discriminator network\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(2048, input_dim=data_dim))\n",
    "    discriminator.add(LeakyReLU(alpha=leaky_alpha))\n",
    "    discriminator.add(Dense(1024))\n",
    "    discriminator.add(LeakyReLU(alpha=leaky_alpha))\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=d_opt)\n",
    "\n",
    "    # setup combined network\n",
    "    gen_dis = Sequential()\n",
    "    gen_dis.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    gen_dis.add(discriminator)\n",
    "    gen_dis.compile(loss='binary_crossentropy', optimizer=g_opt)\n",
    "    return (generator, discriminator, gen_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup experiment\n",
    "max_epochs = 250\n",
    "batch_size = 64\n",
    "latent_space_size = data_dim\n",
    "z_space = get_uniform_space(0, 1, latent_space_size)\n",
    "generator, discriminator, gen_dis = get_mlp_model(latent_space_size)\n",
    "\n",
    "# prepare data\n",
    "gen_labels = np.ones(2*batch_size)\n",
    "gen_labels[:batch_size] = 0\n",
    "gen_disc_labels = np.ones(batch_size)\n",
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "for i in range(max_epochs):\n",
    "    z_samples = z_space(batch_size).astype('float32')\n",
    "    n = np.random.randint(0, data_count-batch_size-1)\n",
    "    x_samples = data[n: n+batch_size].reshape(batch_size, -1)\n",
    "    \n",
    "    g_hist = gen_dis.fit(z_samples, gen_disc_labels, epochs=1, verbose=0)\n",
    "    d_hist = discriminator.fit(np.vstack([generator.predict(z_samples), x_samples]),\n",
    "                               gen_labels, epochs=1, verbose=0)\n",
    "    \n",
    "    g_losses.append(g_hist.history['loss'])\n",
    "    d_losses.append(d_hist.history['loss'])\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        print 'G:', g_hist.history['loss']\n",
    "        print 'D:', d_hist.history['loss']\n",
    "        z_samples = z_space(batch_size).astype('float32')\n",
    "        fakes = generator.predict(z_samples[:8,:])\n",
    "        plot_images(fakes, data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(('g_loss', g_losses), ('d_loss', d_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].reshape(-1).reshape(data_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
