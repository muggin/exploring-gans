{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "\n",
    "from utils import *\n",
    "from skimage import transform, color\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from keras import initializers\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from keras.layers.core import Reshape,Flatten\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data constants\n",
    "max_size = (32, 32)\n",
    "data_files = {'cifar': '../data/cifar.npy',   # cifar data\n",
    "              'mnist': '../data/mnist.npy',   # mnist \n",
    "              'lfw': '../data/lfw.npy',       # labeled faces in the wild\n",
    "              'lfwcpg': '../data/lfwcpg.npy', # labeled faces in the wild, cropped, greyscale\n",
    "              'lfwcpc': '../data/lfwcpc.npy'} # labeled faces in the wild, cropped, color\n",
    "\n",
    "# load and transform data\n",
    "data = np.load(data_files['mnist'])\n",
    "data = data.astype('float32')\n",
    "\n",
    "if data.shape[1:3] >= max_size:\n",
    "    data = np.array([transform.resize(image, max_size, preserve_range=True, order=0) for image in data])\n",
    "\n",
    "# shuffle data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# get data sizes\n",
    "data_count = data.shape[0]\n",
    "data_size = data.shape[1:4]\n",
    "data_dim = reduce(operator.mul, data.shape[1:])\n",
    "\n",
    "# reshape data if necessary\n",
    "data = data.reshape(data_count, *data_size)\n",
    "\n",
    "print 'Loaded data {}'.format(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_axis = 1\n",
    "\n",
    "data_proc = data / 255.\n",
    "\n",
    "# data_reshaped = data.reshape(data_count, -1)\n",
    "# data_mean = np.mean(data_reshaped, axis=statistics_axis, keepdims=True, dtype=np.float64)\n",
    "# data_z_mean = (data_reshaped - data_mean)\n",
    "# data_std = np.std(data_z_mean, axis=statistics_axis, keepdims=True, dtype=np.float64) + 1e-16\n",
    "# data_normed = data_z_mean / data_std\n",
    "# data_proc = data_normed.reshape(data_count, *data_size)\n",
    "\n",
    "print 'Before preproc', data.mean(), data.std()\n",
    "print 'After preproc', data_proc.mean(), data_proc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(data_proc[0].reshape(32,32), cmap='gray')\n",
    "plt.imshow(data_proc[8].reshape(64,64), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Space Generators\n",
    "Factory methods for functions that will generate random samples from the latent space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# latent space generators\n",
    "def get_uniform_space(high, low, space_size):\n",
    "    return lambda batch_size: np.random.uniform(low, high, (batch_size, space_size)).astype('float32')\n",
    "\n",
    "def get_gaussian_space(mean, var, space_size):\n",
    "    return lambda batch_size: np.random.normal(mean, var, (batch_size, space_size)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DCGAN Model\n",
    "Factor method for a Convolutional GAN model.\n",
    "- Generator consists of 1 Dense Input layer followed by 2 Convolutional layers.\n",
    "- Discriminator consists of the same layers but in reversed order. Dropout layers were added to stabilize learning.\n",
    "- Both models use LeakyReLU as their activation functions and Batch Normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conv_model(data_dim, latent_dim):\n",
    "    # setup optimizer\n",
    "    opt = Adam(lr=0.0001, beta_1=0.5)\n",
    "\n",
    "    g_leaky_alpha = 0.05\n",
    "    d_leaky_alpha = 0.1\n",
    "    dropout_p = 0.1\n",
    "\n",
    "    # Generator\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(8*8*128, input_dim=latent_dim, kernel_initializer='random_normal'))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(g_leaky_alpha))\n",
    "    generator.add(Reshape((8,8,128)))\n",
    "    generator.add(UpSampling2D(size=(2, 2)))\n",
    "    generator.add(Conv2D(filters=64,kernel_size=(5,5), padding='same'))\n",
    "    generator.add(BatchNormalization(axis=3))\n",
    "    generator.add(LeakyReLU(g_leaky_alpha))\n",
    "    generator.add(UpSampling2D(size=(2,2)))\n",
    "    generator.add(Conv2D(filters=3,kernel_size=(5,5), padding='same', activation='sigmoid'))\n",
    "    \n",
    "    generator.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "    # Discriminator\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Conv2D(filters=64,kernel_size=(5,5), padding='same', strides=(2,2), \n",
    "                             input_shape=(32,32,3), kernel_initializer='random_normal'))\n",
    "    discriminator.add(BatchNormalization(axis=3))\n",
    "    discriminator.add(LeakyReLU(d_leaky_alpha))\n",
    "    discriminator.add(Dropout(dropout_p))\n",
    "    discriminator.add(Conv2D(filters=128,kernel_size=(5,5), padding='same', strides=(2,2)))\n",
    "    discriminator.add(BatchNormalization(axis=3))\n",
    "    discriminator.add(LeakyReLU(d_leaky_alpha))\n",
    "    discriminator.add(Dropout(dropout_p))\n",
    "    discriminator.add(Flatten())\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "        \n",
    "    # setup combined network\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(latent_dim,))\n",
    "    gan_output = discriminator(generator(gan_input))\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return (generator, discriminator, gan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup experiment\n",
    "max_epochs = 15\n",
    "batch_size = 128\n",
    "latent_dim = 10\n",
    "z_space = get_gaussian_space(0, 1, latent_dim)\n",
    "generator, discriminator, gen_dis = get_conv_model(data_dim, latent_dim)\n",
    "\n",
    "z_samples = z_space(batch_size).astype('float32')\n",
    "fakes = generator.predict(z_samples[:8,:])\n",
    "show_images(fakes, data_size)\n",
    "\n",
    "# prepare data\n",
    "dis_labels = np.ones(2*batch_size) * 0.9# np.random.uniform(0.85, 1.0, 2*batch_size)\n",
    "dis_labels[:batch_size] = 0 \n",
    "gan_labels = np.ones(batch_size)\n",
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "use_batch_norm = True\n",
    "\n",
    "for epoch_ix in xrange(max_epochs):\n",
    "    for batch_ix in tqdm(xrange(0, data_count-batch_size, batch_size)):\n",
    "        \n",
    "        if use_batch_norm:\n",
    "            # train discriminator - true\n",
    "            discriminator.trainable = True\n",
    "            x_samples = data_proc[batch_ix: batch_ix+batch_size]\n",
    "            d_loss_t = discriminator.train_on_batch(\n",
    "                x_samples, dis_labels[batch_size:])\n",
    "\n",
    "            # train discriminator - fake\n",
    "            z_samples = z_space(batch_size).astype('float32')\n",
    "            d_loss_f = discriminator.train_on_batch(\n",
    "                generator.predict(z_samples), dis_labels[:batch_size])\n",
    "            d_loss = (d_loss_t + d_loss_f) / 2\n",
    "        else:\n",
    "            discriminator.trainable = True\n",
    "            x_samples = data_proc[batch_ix: batch_ix+batch_size]\n",
    "            z_samples = z_space(batch_size).astype('float32')\n",
    "            d_loss = discriminator.train_on_batch(\n",
    "                np.vstack((generator.predict(z_samples), x_samples)), dis_labels)\n",
    "\n",
    "        # train generator\n",
    "        z_samples = z_space(batch_size).astype('float32')\n",
    "        discriminator.trainable = False\n",
    "        g_loss = gen_dis.train_on_batch(z_samples, gan_labels)\n",
    "        \n",
    "        # save losses\n",
    "        g_losses.append(g_loss)\n",
    "        d_losses.append(d_loss)\n",
    "    \n",
    "    print 'epoch: {} -- loss G: {} - D: {}'.format(epoch_ix, \n",
    "                                                   g_loss, \n",
    "                                                   d_loss)\n",
    "    z_samples = z_space(batch_size).astype('float32')\n",
    "    fakes = generator.predict(z_samples[:8,:])\n",
    "    show_images(fakes, data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_fig = plot_performance(('g_loss', g_losses), ('d_loss', d_losses))\n",
    "perf_fig.show()\n",
    "\n",
    "# sample generator\n",
    "# z_samples = z_space(batch_size).astype('float32')\n",
    "fakes = generator.predict(z_samples[:8,:])\n",
    "sample_fig = plot_images(fakes, data_size)\n",
    "sample_fig.show()\n",
    "\n",
    "perf_fig.savefig('dcgan-experiments/perf_lat100-alr0_0001-ab10_9-ab20_5-gactlinear-ralpha0_1-dop0_1-bn1-faces-color')\n",
    "sample_fig.savefig('dcgan-experiments/samples_lat100-alr0_0001-ab10_9-ab20_5-gactlinear-ralpha0_1-dop0_1-bn1-faces-color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_samples = z_space(batch_size).astype('float32')\n",
    "fakes = generator.predict(z_samples[:16])\n",
    "sample_fig = plot_images(fakes, data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakes = generator.predict(np.vstack((z_1, z_2, z_3, z_4)))\n",
    "sample_fig = plot_images(-fakes, data_size)\n",
    "sample_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a_ix, b_ix = np.random.randint(0, 127, size=2)\n",
    "endpoints = zip(z_samples[a_ix], z_samples[b_ix])\n",
    "path = np.array([np.linspace(start, stop, 6) for start, stop in endpoints]).T\n",
    "\n",
    "fakes = generator.predict(path)\n",
    "sample_fig = plot_images(fakes, data_size, max_col=6)\n",
    "sample_fig.show()\n",
    "\n",
    "# sample_fig.savefig('dcgan-experiments/trans_lat100-alr0_0001-ab10_9-ab20_5-gactlinear-ralpha0_1-dop0_1-bn1-faces-crop-color')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
